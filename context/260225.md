# 全局

### 🎯 阶段一：架构分层与纯函数核心 (Architecture & Functional Core)
1. **Quincunx 的绝对领域无关性**
   * **挑战**：如何不让底层执行器被具体的业务逻辑（如音轨、乐句）污染。
   * **决策**：严格的 4 层架构（Domain -> Builder -> Quincunx/Lily -> Orchid）。业务层负责把高维概念降维成纯粹的 Node/Edge 交给底层。
2. **"Functional Core, Imperative Shell" 模式的最佳实践**
   * **成果**：你写出了惊艳的 `Lily` 纯函数大脑。
   * **特性**：通过 `Lily.History` 实现 O(1) 零拷贝的时间旅行 (Undo/Redo)；通过 `Graph.topological_sort` 实现了纳秒级的防死锁环检测。

### 🌊 阶段二：受控并发与异构调度 (Orchestration & Batching)
1. **应对设备的异构性（工作站 vs 轻薄本）**
   * **挑战**：重型 AI 模型（声学/声码器）会导致小显存设备 OOM。
   * **决策**：放弃单乐句的“深度优先”贯穿，采用**「屏障批处理 (Barrier Batching)」**。
2. **图的分区与桥接 (Graph Partitioning)**
   * **机制**：在 `Lily.Compiler` 中通过 `Cluster.paint_graph` 进行节点染色。
   * **成果**：将一个大 DAG 切割为多个有序的 Sub-Recipes，并自动推导出悬空端口作为跨集群的 `requires` 和 `exports`。
3. **The Blackboard (黑板机制)**
   * **机制**：在外壳调度器 `RenderEngine` 中，使用生命周期极短的临时字典，在各 Cluster 的批量异步流 (`Task.async_stream`) 之间搬运输出与输入。彻底摊销了模型冷启动时间。

### 💾 阶段三：极致的增量缓存与数据流 (Caching & Data Flow)
1. **Zero-Copy 重载荷流转**
   * **挑战**：Erlang/BEAM VM 不擅长在进程间搬运大张量（Tensor/Audio）。
   * **决策**：`Orchid.Param` 中只传递轻量级的引用指针 `{:ref, Repo, hash_key}`。真实的重数据下放给后端的 ETS/DETS 甚或 Rust NIF 持久化。
2. **Hook 的洋葱模型执行序列**
   * **决策**：严格规定 `[TelemetryHook, DataInterventionHook, CacheHook, CoreHook]` 的执行顺序。
3. **Override 与 Offset 的“大一统” (Aha Moment!)**
   * **挑战**：如何处理用户画的变调曲线等叠加数据（Offset），同时不破坏 Hash 缓存。
   * **决策**：利用 `Orchid.Param` 中的 `type` 字段，将柔性曲线**栅格化**为 1D Tensor 并丢入 Repo。Offset 变成了与普通文本 Override 毫无区别的独立端口输入。**CacheHook 只需要无脑计算引用的 Hash，输入如果改变，缓存自动穿透失效。**

### 🎨 阶段四：Livebook 可视化图景 (Ecosystem & UI)
1. **KinoLily 的受控视图设计**
   * **探讨**：基于 `Kino.JS.Live` 或 Smart Cell 构建所见即所得的节点编辑器。
   * **机制**：前端（如 React Flow / LiteGraph）只负责发事件和渲染 UI，核心拓扑和死锁校验完全交由 Elixir 后端的 `Lily` 毫秒级处理并广播结果。配合原生的 Kino Sliider，实现了极度丝滑的原型调试环境。

### 一句话总结

一个**具备事件溯源能力、零拷贝内存管理、高度设备自适应，且由纯函数控制图拓扑状态的工业级 DAG 执行引擎基础设施**。

---
# 部分难点

## 分簇

> 分簇的设计含义是控制并发粒度。
>
> 我先尝试自己扩展下，虽然最终的设计愿景包括了解决数据的异质性（包括处理过程的异构性），但是一个 `RenderEngine` 的 Task 处理的大概率是一组会生成相同 Recipes 的 Segment 结构体。所以还算是个批处理流水线的 scenario。
>
> 我之前的想法是和 Orchid 的缓存机制挂钩，但是那样太过宽泛了。
>
> 再想不到了。

你的直觉非常敏锐，直接抓住了这个引擎调度最核心的本质：**「受控并发的批处理管线（Controlled Concurrency Batch Pipeline）」**。

你觉得和 `Orchid` 缓存挂钩太宽泛，这是绝对正确的。因为**缓存（Cache）的职责是决定“做不做（Skip or Compute）”**，而 **RenderEngine 的职责是决定“怎么排队做（Schedule & Concurrency）”**。这两个机制应该完全解耦。

在同一个工程里，大量的 `Segment`（乐句）由于共享了相同的 `base_graph`，它们通过 `Lily` 编译出来的 `[Recipe]` 序列在结构上是绝对一致的（比如都是 `Recipe_CPU_Pre` -> `Recipe_GPU_Acoustic` -> `Recipe_GPU_Render`），唯一的区别仅仅是每个 `Segment` 挂载的 `inputs/overrides/offsets` 数据不同。

基于这个前提，我们可以引入一种极其适合 Elixir 的调度模式：**阶段式屏障批处理（Stage-Sequential Barrier Batching）**。

### 核心设计思想：纵切变横切，用时间换空间

假设你有 100 个乐句（Segments），架构上有 2 个阶段（Clusters）：CPU 文本处理，GPU 声学渲染。

*   **纵向（不推荐）**：每个乐句独立跑完 CPU -> GPU。会导致 GPU 频繁地被不同乐句抢占，显存碎片化，甚至死锁。
*   **横向（推荐实现）**：
    1. **开启 CPU 阶段**：并发度设为 16。100 个乐句全部跑完 CPU 获取文本特征。结果存入临时黑板。
    2. **跨越屏障 (Barrier)**：等待所有乐句的 CPU 阶段完成。
    3. **开启 GPU 阶段**：并发度设为 2（防止爆显存）。初始化 GPU 重服务，加载模型。
    4. 100 个乐句拿着刚算出的文本特征，排队跑完 GPU 渲染。
    5. **清理阶段**：卸载模型。

---

### 具体代码落地尝试：The Batch Orchestrator

我们可以利用 Elixir 标准库中最强悍的批处理武器：`Task.async_stream/3` 来实现这个“受控并发”。

你可以将这个 `RenderEngine` 设计成这样：

```elixir
defmodule Quincunx.RenderEngine.BatchOrchestrator do
  @moduledoc """
  宏观调度器。按 Cluster (阶段) 批量处理所有共享相同拓扑的 Segments。
  """
  require Logger

  # 假设我们从系统配置中获知各个 Cluster 的并发能力
  # 后面会设一个全局的 cluster_options 的 module
  @cluster_concurrency %{
    cpu_cluster: System.schedulers_online(), # CPU 密集，全开
    gpu_cluster: 2                           # GPU 显存有限，严格限流
  }

  @doc """
  入参: 
  - `compiled_segments`: 一个列表，元素形如 `%{segment_id: id, bundles: [recipe_bundle]}`
  - `global_inputs`: 当前批次的原始输入（比如最开头的拼音、谱面数据）
  """
  def execute_batch(compiled_segments, global_inputs) do
    # 1. 提取出结构模板（因为拓扑相同，取第一个 Segment 的 recipes 序列作为执行顺序）
    # 形式如: [:cpu_cluster, :gpu_cluster]
    template_bundles = hd(compiled_segments).bundles
    
    # 2. 初始化黑板：为每个 Segment 建立一个独立的 KV 存储
    # 形如: %{ seg_1 => %{ {:port...} => param }, seg_2 => %{ ... } }
    initial_blackboard = init_blackboard_for_segments(compiled_segments, global_inputs)

    # 3. 按阶段（Cluster）进行"横向"遍历
    Enum.reduce_while(template_bundles, initial_blackboard, fn target_bundle, blackboard ->
      cluster_name = target_bundle.recipe.name
      max_con = Map.get(@cluster_concurrency, cluster_name, 1)

      Logger.info(">>> 正在启动阶段: #{cluster_name}, 并发度: #{max_con}")

      # A. 准入前置：可以在这里向全局 WorkerPool 申请加载特定 AI 模型
      # HeavyService.load_model_for(cluster_name)

      # B. 对所有 Segment 执行当前这个 Cluster 的计算
      # async_stream 天生带有背压和并发控制，非常完美
      stage_results =
        Task.async_stream(compiled_segments, fn seg_info ->
          # 找出当前 Segment 对应的具体的 bundle (包含了它特有的 overrides/offsets)
          seg_bundle = Enum.find(seg_info.bundles, &(&1.recipe.name == cluster_name))
          
          # 从黑板中拔出当前 Segment 需要的输入
          seg_local_blackboard = Map.fetch!(blackboard, seg_info.segment_id)
          inputs = extract_requires(seg_local_blackboard, seg_bundle.requires)

          # 执行 Orchid 并利用 Hook 注入它的 Overrides
          run_opts = [
             baggage: %{overrides: seg_bundle.overrides, offsets: seg_bundle.offsets},
             # 注意 Hook 顺序：先数据干预，后查缓存
             hooks: [Quincunx.Hooks.DataIntervention, Quincunx.Hooks.CacheHook]
          ]
          
          case Orchid.run(seg_bundle.recipe, inputs, run_opts) do
            {:ok, outputs} -> 
              {:ok, seg_info.segment_id, seg_bundle.exports, outputs}
            {:error, err} -> 
              {:error, seg_info.segment_id, err}
          end
        end, max_concurrency: max_con, timeout: :infinity)
        # 必须显式枚举才会真正执行 Stream
        |> Enum.to_list() 

      # C. 屏障同步（Barrier Sync）与 状态更新
      # 检查这一批 100 个有没有报错的，如果都成功，把产物写回各自的黑板
      case check_and_commit_results(stage_results, blackboard) do
        {:ok, updated_blackboard} -> 
          # HeavyService.unload_model_for(cluster_name)
          {:cont, updated_blackboard}
          
        {:error, reason} -> 
          Logger.error("批处理在 #{cluster_name} 阶段崩溃: #{inspect(reason)}")
          {:halt, {:error, reason}}
      end
    end)
  end

  # ============ 辅助纯函数 ============

  defp extract_requires(local_blackboard, requires_keys) do
    Enum.map(requires_keys, &Map.fetch!(local_blackboard, &1))
  end

  defp check_and_commit_results(stage_results, blackboard) do
    # 只要有一个 {:error, ...}，整个批处理就宣告失败 (或者你也可以做容错跳过)
    failed = Enum.find(stage_results, fn
      {:ok, {:error, _id, _err}} -> true
      {:exit, _} -> true # 并发崩溃
      _ -> false
    end)

    if failed do
      {:error, failed}
    else
      # 全部成功，合并导出数据
      new_blackboard =
        Enum.reduce(stage_results, blackboard, fn {:ok, {:ok, seg_id, export_keys, outputs}}, acc ->
          # 将当前 Orchid 跑出来的局部结果，打包成 Map 塞回黑板
          # 这里假定 outputs 也是按照 export_keys 的顺序对齐的
          export_map = Enum.zip(export_keys, outputs) |> Map.new()
          
          # 更新这个 segment 的黑板
          Map.update!(acc, seg_id, &Map.merge(&1, export_map))
        end)

      {:ok, new_blackboard}
    end
  end
end
```

### 这个调度的绝妙之处：

1. **精准的并发控制**：纯 CPU 阶段可以把机器的 16 个核心跑满；到了 GPU 阶段，并发被死死限制在 `2`。这从根本上杜绝了推理显存溢出（OOM）。
2. **重服务冷启动的绝对摊销**：你在 `Cluster` 开始前加载模型（哪怕要花 5 秒钟），然后 100 个乐句复用这个模型跑完，再释放它。这个时间成本被极致摊配了。
3. **与缓存机制互相成就**：回到我们之前聊的，缓存是用来“Skip”的。虽然我们在跑“批处理”，但只要某个 `Segment` 的输入和 `Baggage(overrides)` 计算出来的 Hash 在持久化（DETS）或内存（ETS）中命中，`Orchid` 在底下依然会通过 `CacheHook` **1 毫秒瞬间返回**。
   * 这意味着：用户只修改了第 99 个乐句的某个音高（Offset）。点击渲染时，虽然 100 个乐句通过上述代码一起被扔进了 `Task.async_stream`，但前 98 个会因为缓存瞬间跑完，资源立刻倾斜给第 99 个重新推理计算。

这个“横切”加上 `Task.async_stream` 的设计，不仅代码量小得惊人，而且非常符合 Elixir 函数式数据流的直觉。你觉得这个处理“异质设备的受控并发”的设计如何？

## 数据本体保存 vs 跨 Recipe 生命周期的保存

> 可否再说一下黑板和缓存的关系呢？我们对于缓存的 impl 好像一直都没有落地。

这是一个绝对关键的灵魂拷问！理清这两者的关系，整个系统的任督二脉才算彻底打通。

为了让你一目了然，我们可以用一个工厂的隐喻来界定它们的边界，然后再把缺失的 **「Cache 落地实现」** 补齐。

### 1. 概念解耦：黑板（传送带） vs 缓存（仓库）

*   **黑板 (Blackboard)** = **跨配方数据传送带（瞬时状态）**
    *   **生命周期**：极短。从按下了“播放键”开始，到 100 个乐句跑完发出声音结束，黑板就被销毁了。
    *   **形态**：映射关系是 `PortKey -> Param`（例如 `{:port, :acoustic, :mel} -> %Param{}`）。
    *   **职责**：因为 Lily 把大图切成了 `Cluster_CPU` 和 `Cluster_GPU`，黑板的唯一作用就是让 `GPU` 启动时，能拿到 `CPU` 刚才算出来的输出参数。**黑板不在乎数据是怎么来的**（不管是硬算的，还是从缓存里捞的，黑板只负责当快递员）。
*   **缓存 (Cache)** = **增量计算的记忆仓库（持久状态）**
    *   **生命周期**：极长。甚至跨越软件的多次启动（存盘在 DETS/文件系统里）。
    *   **形态**：映射关系是 `Computation_Signature (Hash) -> Param`。
    *   **职责**：决定“算”还是“跳过”。拦截重型步骤，通过指纹比对，如果一模一样，直接变出结果扔给黑板，从而保护 CPU/GPU 资源。

---

### 2. 缓存 (Cache) 的真正落地实现

要让缓存跑起来，我们需要两部分：**哈希签名生成器** 和 **拦截钩子 (CacheHook)**。

#### A. 核心难点：怎么算 Hash？
计算 Hash 最忌讳的就是“把几个 G 的张量数据取出来去算 Hash”。我们在前面的讨论中确立过：`Param` 的 `payload` 里往往存放的是引用（`{:ref, repo, key}`）。
此外，我们刚才在 `DataInterventionHook` 里把 offset 写进了 `metadata._orchid_offset`，**这个 Offset 必须参与 Hash 计算**，否则用户拉动了变调滑块，系统还会返回旧的音频！

```elixir
defmodule Quincunx.Cache.Signature do
  @moduledoc "为单次计算生成确定性的指纹 (Hash)"

  def generate_hash(impl, inputs, opts) do
    # 1. 提取所有输入的“特征指纹”
    inputs_fingerprints = Enum.map(inputs, &extract_param_fingerprint/1)
    
    # 2. 清理 opts 中不稳定的部分（比如 telemetry_meta 或运行时 PID）
    stable_opts = Keyword.drop(opts, [:__reporter_ctx__, :baggage, :hooks])

    # 3. 组合成 Erlang Term
    term_to_hash = {impl, inputs_fingerprints, stable_opts}
    
    # 4. 生成极速的 256 位 Hash
    :crypto.hash(:sha256, :erlang.term_to_binary(term_to_hash))
    |> Base.encode16(case: :lower)
  end

  # =--- 多态提取 Param 特征 ---=
  
  # 情况 A：数据如果是个胖引用（NIF 句柄 / 文件路径），我们只 Hash 它的引用 Key！极速！
  defp extract_param_fingerprint(%Orchid.Param{payload: {:ref, _repo, ref_key}} = param) do
    {ref_key, Map.get(param.metadata, :_orchid_offset)}
  end
  
  # 情况 B：数据如果是轻量的原子、数字或短文本，直接 Hash 它的裸值
  defp extract_param_fingerprint(%Orchid.Param{payload: raw_data} = param) 
       when is_binary(raw_data) or is_number(raw_data) or is_atom(raw_data) do
    {raw_data, Map.get(param.metadata, :_orchid_offset)}
  end

  # 情况 C：其他容错（比如嵌套结构），用 Erlang 自带的哈希
  defp extract_param_fingerprint(other) do
    :erlang.phash2(other)
  end
end
```

#### B. CacheHook (缓存短路拦截器)

在 `Orchid.Runner` 的洋葱模型里，**如果 Hook 不调用 `next.(ctx)`，它就会直接打断管线**并原路返回。这正是完美实现缓存短路的地方！

```elixir
defmodule Quincunx.Hooks.Cache do
  @behaviour Orchid.Runner.Hook
  alias Quincunx.Cache.Signature
  alias Quincunx.GlobalCacheRepo # 假设这是你的 ETS/DETS 存储进程

  require Logger

  @spec call(Orchid.Runner.Context.t(), Orchid.Runner.Hook.next_fn()) ::
          Orchid.Runner.Hook.hook_result()
  def call(ctx, next) do
    # 1. 计算当前步骤的纯粹计算签名
    step_hash = Signature.generate_hash(ctx.step_implementation, ctx.inputs, ctx.step_opts)

    # 2. 查仓库
    case GlobalCacheRepo.get(step_hash) do
      {:ok, cached_output_params} ->
        Logger.debug("Cache HIT! 命中缓存短路: #{ctx.step_implementation}")
        # 【核心技巧】：不调用 next.(ctx)，直接返回成功的 tuple！
        # 这会让底层的 Hooks.Core（真正调 NIF/跑模型的地方）被彻底跳过！
        # 但外层的 Telemetry 依然会收到 {:ok, result}，可观测性完美保留。
        {:ok, cached_output_params}

      :error ->
        Logger.debug("Cache MISS: #{ctx.step_implementation}，执行深度计算...")
        
        # 没命中，必须硬算。调用 next.(ctx) 走向 CoreHook
        case next.(ctx) do
          {:ok, outputs} ->
            # 计算成功后，把结果写回仓库，留给下次用
            GlobalCacheRepo.put(step_hash, outputs)
            {:ok, outputs}
            
          other_result ->
            # 如果底层计算报错了，直接透传，千万别缓存错误结果
            other_result
        end
    end
  end
end
```

### 3. 把它们串起来的宏观图景

让我们回放一下 `Segment` 第 99 乐句在 Engine 里的奇妙旅程：

1.  **用户推了一下变调滑块**。
2.  `Lily` 瞬间折叠历史，吐出了带有 `offsets: %{{:port, :vocoder, :mel} => +2.0}` 的配方。
3.  `BatchOrchestrator` 把 100 个乐句扔进流水线。
4.  **【黑板运作】**：到了 Vocoder (声码器) 节点，黑板把上一步声学模型生成的 `Param (payload: {:ref, repo, "mel_tensor_abc"})` 拔出来，交给了 Orchid。
5.  **【DataInterventionHook】**：拦截，发现 `baggage` 里有 `+2.0`。它不会去改张量，而是把 `metadata: %{_orchid_offset: 2.0}` 塞进了这个 `Param` 里。
6.  **【CacheHook】**：开始算 Hash。
    *   乐句 1 到 98，`offset` 还是缺省的 `nil`。`Signature.generate_hash` 算出来的指纹在仓库里找得到。**Hit！跳过 GPU 渲染，瞬间返回音频。**
    *   乐句 99，因为 `offset` 变成了 `2.0`，计算出的 `step_hash` 变了！在仓库里找不到。**Miss！** 放行到 `Hooks.Core` 开始硬算这一个乐句的 GPU 渲染。算完后，把变调后的音频写回仓库。
7.  **【黑板运作】**：Orchid 返回了新算好的音频 Param，黑板又把它装载上去，最终存进工程。

你看，**黑板负责跨图搬运引用，Hook 负责篡改属性，Cache 负责由属性生成指纹并拦截**。各司其职，没有一点相互污染。这个 `impl` 落地设计是否解答了你心中的疑惑？

---
# 分主题

### 📦 Prompt 1: The Render Pipeline & Blackboard (渲染管线与黑板机制)

**背景上下文**：

我正在使用 Elixir 开发一个基于 DAG 的音乐/多媒体批处理管线（Quincunx/Orchid）。纯函数核心 `Lily` 已经能将用户编辑历史叠合并切分为多个 `Orchid.Recipe`（如 CPU 簇、GPU 簇），并带有跨图的 `requires` 和 `exports` 端口声明。

【`Lily` 文档以及源码】

**本次 Session 目标**：

设计并实现 `Quincunx.RenderEngine`。

1. 它需要接收 `Lily` 吐出的 `[Recipe]` 序列和 100 个需要被并发渲染的 `Segments`（乐句）。
2. 使用 `Task.async_stream` 实现**阶段式屏障批处理（Stage-Sequential Batching）**。
3. 设计并落地 **黑板机制 (Blackboard)**：一个生命周期仅限于单次渲染的键值对字典。它负责收集上一个 Recipe 阶段产出的 `Orchid.Param`，并在下一个 Recipe 阶段被唤醒时，根据 `requires` 准确地把参数喂给执行器。不涉及具体的业务计算，纯粹做数据引用的搬运工。

请从 Elixir OTP 和数据流的角度，为我搭建这个 Engine 的核心流转代码。

### 📦 Prompt 2: Cache Implementation & Hooks (增量缓存落地与钩子机制)

【Orchid 的文档以及代码（`Orchid.Param`、`Orchid.Runner` 及相关钩子）】

**背景上下文**：

在我的工作流引擎（Orchid）中，`Orchid.Runner` 采用了基于插件的洋葱模型 Hook 机制。我现在需要完成 **增量缓存短路 (Cache Short-circuiting)** 机制的落地。

**本次 Session 目标**：

1. 实现 `Quincunx.Hooks.CacheHook`。要求在这个 Hook 中，基于 `hash(step_impl, inputs, opts)` 去查询缓存后端，命中则打断 Hook 链直接返回（跳过昂贵计算），未命中则调用 `next.(ctx)` 计算并回写缓存。
2. 实现 **签名生成器 (Hash Generator)**。特别注意：`inputs` 中的 `Orchid.Param.payload` 往往是 `{:ref, Repo, ref_key}`，计算 Hash 时**绝对不能** Fetch 真实的重数据，只能 Hash 引用指针和相关元数据。
3. 讨论后端的落地：如何利用 `Orchid.Repo` 行为（Behaviour）结合 ETS（一级内存加速）和 DETS（二级工程持久化）来存储不同生命周期的中间产物。
 
请给出缓存 Hook 的实现逻辑，并重点处理 Elixir `:crypto.hash` 针对大载荷引用的边界情况。

### 📦 Prompt 3: Polymorphic Interventions (多态数据干预与 Defprotocol)

【项目整体样貌】

【Orchid 的文档以及代码（`Orchid.Param`、`Orchid.Runner` 及相关钩子、`Lily` 的 Demo）】

**背景上下文**：

我的系统允许用户在 UI 上对生成的渲染数据进行破坏性修改（Override）或叠加（Offset）。目前，这些干预操作都被抽象为 `Orchid.Param` 流入底层。

**本次 Session 目标**：

设计一套处理「异质数据干预」的多态体系：

1. **曲线与栅格化**：用户手绘的 1D 工具曲线，在进入系统时被栅格化为 Tensor，并带有 `type: :tensor_1d_curve`。
2. **基本类型**：如字符串、短整数，可以直接放在 `%Param{payload: raw_data}` 中。
3. **大数据引用**：对重型音频张量，使用 `payload: {:ref, NifRepo, ref_key}` 显式读写。
4. 我计划引入一个 Elixir `defprotocol`（例如 `Quincunx.DataResolver`），让调度器在处理这些不同的 `Param` 类型时，能够自动解包或加载数据，而不污染通用引擎的逻辑。

请协助我设计 `DataInterventionHook` 以及这个 Protocol 的接口，使得拦截并篡改输入的逻辑极度优雅。


### 📦 Prompt 4: Heavy Service Management (重型服务与资源潮汐加载)

【前置条件： `RenderEngine` 的样貌基本确定】

【将粘贴相关代码】

**背景上下文**：

我的批处理管线包含 AI 语音生成的声学模型和声码器模型，它们通过 Rust/C++ NIF 接入 Elixir。由于目标设备的显存有限（如 4GB），我不能同时将所有模型常驻内存。

**本次 Session 目标**：

设计 `Quincunx.HeavyServiceManager`。

1. 它需要配合 `RenderEngine` 的阶段式批处理工作。当引擎执行到 `Cluster_GPU_Acoustic` 阶段时，Manager 负责向底层 NIF 发送加载对应模型的指令。
2. 只有当 100 个并发的计算任务全部穿越该集群阶段的同步屏障（Barrier）后，才执行 `unload` 卸载显存。
3. 探讨 Elixir 层如何利用 `NimblePool` 或原生 `GenServer` 创建与 C/Rust NIF 通信的常驻工作进程（Worker Pool），以及在发生进程 Crash 时的重试/容错恢复机制。

请以 Elixir 应对高并发和 C/Node 交互的最佳实践，提供资源池防洪和模型挂载方案。

### 📦 Prompt 5: KinoLive Frontend Integration (基于 Livebook 的可视化编辑器)

【将提供 Lily 的全部代码 ~500 lines】

**背景上下文**：

我拥有一个纯函数式的 DAG 状态机 `Lily`。它基于 Event Sourcing（Undo/Redo），能够毫秒级运算图拓扑并输出渲染配方。现在，我希望在 Elixir Livebook 中开发一个基于 `Kino.JS.Live` 的可视化前端 `KinoLily`。

**本次 Session 目标**：

1. 在 Livebook 侧编写一个 `Kino.JS.Live` 组件，作为前后端的双向 WebSocket 桥梁。
2. 前端（拟采用 React Flow 或 LiteGraph.js）作为纯粹的“受控视图（Controlled Component）”，仅负责渲染 Node/Edge 和分发拖拽连线事件。
3. 处理核心反馈循环：JS 连线 $\to$ Elixir 接收事件并调用 `Lily.apply_operation` $\to$ `Lily` 侦测是否死锁（环检测） $\to$ Elixir 广播合法的图数据给 JS 触发 UI 刷新，并配合 Kino Slider 实现参数调整的实时反馈。

请帮我梳理 Kino.JS.Live 的消息契约（Payload/Event API），并提供 JS 与 Elixir 之间解耦互动的核心架子代码。
